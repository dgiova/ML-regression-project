{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries necessary for this project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 4\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n",
    "from scipy import stats\n",
    "import warnings\n",
    "def ignore_warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = ignore_warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train data size before dropping Id feature is : (1460, 81) \n",
      "The test data size before dropping Id feature is : (1459, 80) \n",
      "\n",
      "The train data size after dropping Id feature is : (1460, 80) \n",
      "The test data size after dropping Id feature is : (1459, 79) \n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "#check the numbers of samples and features\n",
    "print(\"The train data size before dropping Id feature is : {} \".format(train.shape))\n",
    "print(\"The test data size before dropping Id feature is : {} \".format(test.shape))\n",
    "\n",
    "#Save the 'Id' column\n",
    "train_ID = train['Id']\n",
    "test_ID = test['Id']\n",
    "\n",
    "#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\n",
    "train.drop(\"Id\", axis = 1, inplace = True)\n",
    "test.drop(\"Id\", axis = 1, inplace = True)\n",
    "\n",
    "#check again the data size after dropping the 'Id' variable\n",
    "print(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) \n",
    "print(\"The test data size after dropping Id feature is : {} \".format(test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train[\"SalePrice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1460"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1459"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "486"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 50% cutoff point\n",
    "cutoff = len(data) // 3\n",
    "cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_drop = []\n",
    "features_filled_std = {}\n",
    "for col in data.columns.values:\n",
    "    if data[col].isna().sum() > cutoff:\n",
    "        data.drop(col, axis=1, inplace=True)\n",
    "        features_drop.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features drop: ['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature']\n"
     ]
    }
   ],
   "source": [
    "print(\"features drop:\", features_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data.columns.values\n",
    "numeric = []\n",
    "categoric = []\n",
    "for col in features:\n",
    "    if data[col].dtype == 'object':\n",
    "        categoric.append(col)\n",
    "    else:\n",
    "        numeric.append(col)\n",
    "numeric.remove('SalePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform categorical features in dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reindex(sorted(data.columns), axis=1)\n",
    "for category in categoric:\n",
    "    series = data[category]\n",
    "    dummies = pd.get_dummies(series, prefix=category)\n",
    "    data = pd.concat([data, dummies], axis=1)\n",
    "data.drop(categoric, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill na in numeric categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in numeric:\n",
    "    if data[num].isna().sum() == 0:\n",
    "        continue\n",
    "    else:\n",
    "        data[num].fillna(data[num].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1460 entries, 0 to 1459\n",
      "Columns: 271 entries, 1stFlrSF to SaleCondition_Partial\n",
      "dtypes: float64(3), int64(34), uint8(234)\n",
      "memory usage: 755.7 KB\n"
     ]
    }
   ],
   "source": [
    "# check dataset\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDum = test.copy()\n",
    "testDum = testDum.reindex(sorted(testDum.columns), axis=1)\n",
    "testDum.drop(features_drop, axis=1, inplace=True)\n",
    "for category in categoric:\n",
    "    series = testDum[category]\n",
    "    dummies = pd.get_dummies(series, prefix=category)\n",
    "    testDum = pd.concat([testDum, dummies], axis=1)\n",
    "\n",
    "for num in numeric:\n",
    "    if testDum[num].isna().sum() == 0:\n",
    "        continue\n",
    "    else:\n",
    "        testDum[num].fillna(testDum[num].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testDum.drop(dropped_corr, axis=1, inplace=True)\n",
    "temp = data[numeric]\n",
    "std_devs = temp.std(axis=0)\n",
    "means = temp.mean(axis=0)\n",
    "norm = (testDum[numeric].sub(means,axis=1)).div(std_devs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MasVnrArea\n",
      "BsmtFinSF2\n",
      "LowQualFinSF\n",
      "EnclosedPorch\n",
      "3SsnPorch\n",
      "MiscVal\n"
     ]
    }
   ],
   "source": [
    "outlie = []\n",
    "for col in norm.columns.values:\n",
    "    if sum(norm[col] > 5) > 5:\n",
    "        outlie.append(col)\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDum.drop(outlie, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = testDum.drop(categoric, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_features = Xtest.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[true_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score for trees 250 is: 0.9070798256174872\n",
      "score for trees 300 is: 0.9135624822980495\n",
      "score for trees 350 is: 0.919113111467827\n",
      "score for trees 400 is: 0.9152713390050683\n",
      "score for trees 450 is: 0.9153292456673144\n",
      "score for trees 500 is: 0.9137634724190284\n"
     ]
    }
   ],
   "source": [
    "best_trees = 0\n",
    "score = 0\n",
    "for i in [250, 300, 350, 400, 450, 500]:\n",
    "    clf = ensemble.GradientBoostingRegressor(n_estimators = i, max_depth = 3, min_samples_split = 2,\n",
    "                                             learning_rate = 0.1, loss = 'huber')\n",
    "    clf.fit(X_train, y_train)\n",
    "    temp = clf.score(X_val, y_val)\n",
    "    print(\"score for trees \" + str(i) +\" is: \" + str(temp))\n",
    "    if temp > score:\n",
    "        best_trees = i\n",
    "        score = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score for depth 2 is: 0.9310828772894992\n",
      "score for depth 3 is: 0.9066226340874908\n",
      "score for depth 4 is: 0.9108830306978162\n",
      "score for depth 5 is: 0.9090833955433468\n",
      "score for depth 6 is: 0.8861720169538837\n"
     ]
    }
   ],
   "source": [
    "best_depth = 0\n",
    "score = 0\n",
    "for i in [2, 3, 4, 5, 6]:\n",
    "    clf = ensemble.GradientBoostingRegressor(n_estimators = best_trees, max_depth = i, min_samples_split = 2,\n",
    "                                             learning_rate = 0.1, loss = 'huber')\n",
    "    clf.fit(X_train, y_train)\n",
    "    temp = clf.score(X_val, y_val)\n",
    "    print(\"score for depth \" + str(i) +\" is: \" + str(temp))\n",
    "    if temp > score:\n",
    "        best_depth = i\n",
    "        score = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score for min sample split 2 is: 0.9245000374842152\n",
      "score for min sample split 3 is: 0.9330010809079379\n",
      "score for min sample split 4 is: 0.9313051245240195\n",
      "score for min sample split 5 is: 0.9296100703353748\n",
      "score for min sample split 6 is: 0.9311054726462186\n",
      "score for min sample split 7 is: 0.9279808535683033\n"
     ]
    }
   ],
   "source": [
    "best_sample_split = 0\n",
    "score = 0\n",
    "for i in [2, 3, 4, 5, 6, 7]:\n",
    "    clf = ensemble.GradientBoostingRegressor(n_estimators = best_trees, max_depth = best_depth, min_samples_split = i,\n",
    "                                             learning_rate = 0.1, loss = 'huber')\n",
    "    clf.fit(X_train, y_train)\n",
    "    temp = clf.score(X_val, y_val)\n",
    "    print(\"score for min sample split \" + str(i) +\" is: \" + str(temp))\n",
    "    if temp > score:\n",
    "        best_sample_split = i\n",
    "        score = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best number of estimators: 350\n",
      "Best depth: 2\n",
      "Best min sample split: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"Best number of estimators:\", best_trees)\n",
    "print(\"Best depth:\", best_depth)\n",
    "print(\"Best min sample split:\", best_sample_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=2, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=3, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=350, n_iter_no_change=None, presort='auto',\n",
       "             random_state=None, subsample=1.0, tol=0.0001,\n",
       "             validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingRegressor(n_estimators = best_trees, max_depth = best_depth, \n",
    "                                         min_samples_split = best_sample_split,\n",
    "                                         learning_rate = 0.1, loss = 'ls')\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = 'GBR_best.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RIDGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha Value:  15\n"
     ]
    }
   ],
   "source": [
    "rcv = linear_model.RidgeCV(alphas=(12,13,14,15,16,17,18),\n",
    "                           cv=5)\n",
    "rcv.fit(X, y)\n",
    "print (\"Alpha Value: \", (rcv.alpha_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('robustscaler', RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,\n",
       "       with_scaling=True)), ('elasticnetcv', ElasticNetCV(alphas=(0.001, 0.0009, 0.0008, 0.0007, 0.0006, 0.0005),\n",
       "       copy_X=True, cv=5, eps=0.001, fit_intercept=True, l1_ratio=0.5,\n",
       "       max_iter=1000, n_alphas=100, n_jobs=None, normalize=False,\n",
       "       positive=False, precompute='auto', random_state=None,\n",
       "       selection='cyclic', tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecv = make_pipeline(RobustScaler(), linear_model.ElasticNetCV(alphas=(0.001,0.0009,0.0008,0.0007, 0.0006, 0.0005),\n",
    "                           cv=5))\n",
    "ecv.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predR = rcv.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predE = ecv.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Ridge_best.sav'\n",
    "pickle.dump(rcv, open(filename, 'wb'))\n",
    "\n",
    "filename = 'ENet_best.sav'\n",
    "pickle.dump(ecv, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUBMIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_comb = 1/2 * y_pred + 1/4 * y_predR + 1/4* y_predE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create submission file\n",
    "sub = pd.DataFrame()\n",
    "sub['Id'] = test_ID\n",
    "sub['SalePrice'] = y_comb\n",
    "sub.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([111094.60291707, 159413.13451673, 177492.22590747, ...,\n",
       "       160585.9437055 , 113074.09831543, 232392.54925399])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
